{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bu3MsdATTg5"
      },
      "source": [
        "# DQN: Reinforcement Learning on Atari Games\n",
        "\n",
        "Created by Paul Scott<br>\n",
        "MSE Computer and Information Science<br>\n",
        "University of Pennsylvania<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTtge015eNb_"
      },
      "source": [
        "# References\n",
        "\n",
        "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "* https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "* https://github.com/mgbellemare/Arcade-Learning-Environment\n",
        "* https://github.com/kenjyoung/MinAtar\n",
        "* https://towardsdatascience.com/double-deep-q-networks-905dd8325412"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ1oZGlbYNG9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2gMROoWp6OF"
      },
      "source": [
        "Download required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJAdFbeDp7qY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/kenjyoung/MinAtar.git\n",
        "%pip install ./MinAtar\n",
        "%pip install --upgrade gym\n",
        "%pip install ale-py\n",
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFzAu6a-YdDm"
      },
      "source": [
        "Download ROMs for ALE environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRrNVDg5OoAD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://atariage.com/2600/roms/SpaceInvaders.zip\n",
        "!wget https://www.atariage.com/2600/roms/MsPacMan.zip\n",
        "!wget https://www.atariage.com/2600/roms/Breakout.zip\n",
        "!wget https://atariage.com/2600/roms/VideoOlympics.zip\n",
        "\n",
        "!mkdir roms/\n",
        "!unzip SpaceInvaders.zip -d roms/\n",
        "!unzip MsPacMan.zip -d roms/\n",
        "!unzip Breakout.zip -d roms/\n",
        "!unzip VideoOlympics.zip -d roms/\n",
        "!mv roms/SPCINVAD.BIN roms/spaceinvaders.bin\n",
        "!mv roms/MSPACMAN.BIN roms/mspacman.bin\n",
        "!mv roms/Breakout.bin roms/breakout.bin\n",
        "!mv roms/Vid_olym.bin roms/pong.bin\n",
        "\n",
        "!ale-import-roms roms/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCMw20MnYb5W"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQLvpYpHYRmj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from ale_py import ALEInterface\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, colors\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from IPython import display\n",
        "import random\n",
        "from PIL import Image\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import gym\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%matplotlib inline \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "plt.rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEsqgJD47Q0"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSkrRzG96g6"
      },
      "source": [
        "Define base environment class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhZuL39z7JOd"
      },
      "outputs": [],
      "source": [
        "class AbstractEnvironment():\n",
        "  def __init__(self, num_actions, state_shape, cmap=None, aspect=None):\n",
        "    self.num_actions = num_actions\n",
        "    self.state_shape = state_shape\n",
        "    self.num_channels = state_shape[1]\n",
        "    self.cmap = cmap\n",
        "    self.aspect = aspect\n",
        "    self.crop = None\n",
        "\n",
        "  def reset(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, action, skip=0):\n",
        "    pass\n",
        "  \n",
        "  def render(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDgm7NVf6KJo"
      },
      "source": [
        "Define ALE environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC0liyHA4a2N"
      },
      "outputs": [],
      "source": [
        "class ALEEnvironment(AbstractEnvironment):\n",
        "  def __init__(self, game, frame_stack_size=4, initial_action=None):\n",
        "    self.env = ALEInterface()\n",
        "    self.env.loadROM(game)\n",
        "    self.env.setFloat('repeat_action_probability', 0)\n",
        "    self.frame_stack = deque()\n",
        "    self.action_space = self.env.getMinimalActionSet()\n",
        "    self.frame_stack_size = frame_stack_size\n",
        "    self.initial_action = initial_action\n",
        "    super().__init__(len(self.action_space), (1, frame_stack_size, 84, 84), aspect='auto')\n",
        "\n",
        "  def reset(self, initial_delay=0):\n",
        "    self.env.reset_game()\n",
        "    if self.initial_action:\n",
        "      self.env.act(self.action_space[self.initial_action])\n",
        "    for _ in range(initial_delay):\n",
        "      self.env.act(0)\n",
        "    self.frame_stack = deque([torch.zeros((84, 84))] * (self.frame_stack_size - 1) + [self.get_frame()])\n",
        "    return self.format_state(self.frame_stack)\n",
        "\n",
        "  def step(self, action, skip=0):\n",
        "    lives = self.env.lives()\n",
        "    reward = self.env.act(self.action_space[action])\n",
        "    for _ in range(skip):\n",
        "      reward += self.env.act(self.action_space[action])\n",
        "    self.frame_stack.popleft()\n",
        "    self.frame_stack.append(self.get_frame())\n",
        "    next_state = self.format_state(self.frame_stack)\n",
        "    done = self.env.lives() != lives or self.env.game_over()\n",
        "    return next_state, reward, done\n",
        "\n",
        "  def render(self):\n",
        "    return self.env.getScreenRGB()\n",
        "\n",
        "  def format_state(self, state):\n",
        "    return torch.stack(tuple(state)).unsqueeze(0).to(device)\n",
        "\n",
        "  def get_frame(self):\n",
        "    screen = Image.fromarray(self.env.getScreenGrayscale())\n",
        "    if self.crop:\n",
        "      screen = screen.crop(self.crop)\n",
        "    screen = screen.resize((84, 84), Image.BILINEAR)\n",
        "    screen = torch.Tensor(np.asarray(screen)) / 255.0\n",
        "    return screen.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-v5octU6L5W"
      },
      "source": [
        "Define MinAtar environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxVR23X-53yl"
      },
      "outputs": [],
      "source": [
        "class MinAtarEnvironment(AbstractEnvironment):\n",
        "  def __init__(self, game):\n",
        "    self.env = gym.make(f'MinAtar/{game}')\n",
        "    state_shape = self.format_state(self.env.render(mode='array')).shape\n",
        "    cmap = sns.color_palette(\"cubehelix\", state_shape[1])\n",
        "    cmap.insert(0, (0, 0, 0))\n",
        "    cmap = colors.ListedColormap(cmap)\n",
        "    super().__init__(self.env.action_space.n, state_shape, cmap=cmap)\n",
        "\n",
        "  def reset(self):\n",
        "    return self.format_state(self.env.reset())\n",
        "\n",
        "  def step(self, action, skip=None):\n",
        "    next_state, reward, done, _ = self.env.step(action)\n",
        "    next_state = self.format_state(next_state)\n",
        "    return next_state, reward, done\n",
        "\n",
        "  def render(self):\n",
        "    return np.amax(self.env.render(mode='array') * \\\n",
        "        np.reshape(np.arange(self.num_channels) + 1, (1, 1, -1)), 2) + 0.5\n",
        "\n",
        "  def format_state(self, state):\n",
        "    return torch.tensor(state, device=device).permute(2, 0, 1).unsqueeze(0).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQnhQGxRBIr4"
      },
      "source": [
        "Select environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rqFjgx45P8B"
      },
      "outputs": [],
      "source": [
        "from ale_py.roms import Pong\n",
        "env = ALEEnvironment(Pong, 2)\n",
        "\n",
        "# env = MinAtarEnvironment('Breakout-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drlWAaJDLcrS"
      },
      "source": [
        "Crop and preview game (ALE only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "TFDFMC5jLeKy",
        "outputId": "c41dffb2-443d-43da-87bc-369e0e403ebb"
      },
      "outputs": [],
      "source": [
        "if type(env) == ALEEnvironment:\n",
        "  width = env.env.getScreenDims()[1]\n",
        "  height =  env.env.getScreenDims()[0]\n",
        "  env.crop = (0, 34, width, height - 16)\n",
        "\n",
        "  plt.imshow(env.get_frame(), cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abiCnasNYuc3"
      },
      "source": [
        "# Deep Q-Network (DQN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIJRriPC4MPJ"
      },
      "source": [
        "Define ALE DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26kfpBcb3LKh"
      },
      "outputs": [],
      "source": [
        "class ALEDQN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(env.num_channels, 32, 8, stride=4)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "    self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "    self.fc1 = nn.Linear(64*7*7, 512)\n",
        "    self.fc2 = nn.Linear(512, env.num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 3x84x84\n",
        "    x = F.relu(self.conv1(x))\n",
        "    \n",
        "    # 32x20x20\n",
        "    x = F.relu(self.conv2(x))\n",
        "\n",
        "    # 64x9x9\n",
        "    x = F.relu(self.conv3(x))\n",
        "\n",
        "    # 64x7x7\n",
        "    x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF8yenhP4N8j"
      },
      "source": [
        "Define MinAtar DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7lI_3jZ3ckJ"
      },
      "outputs": [],
      "source": [
        "class MinAtarDQN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(env.num_channels, 32, 3, padding='same')\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding='same')\n",
        "    self.fc1 = nn.Linear(64*2*2, 512)\n",
        "    self.fc2 = nn.Linear(512, env.num_actions)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 4x10x10\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.maxpool(x)\n",
        "    \n",
        "    # 32x5x5\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # 64x2x2\n",
        "    x = F.relu(self.fc1(x.flatten(start_dim=1)))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLbWM3ZN4Pm8"
      },
      "source": [
        "Select DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKbzSQNr4FmV",
        "outputId": "8616e396-42fd-4401-ecb2-1084ad22bd14"
      },
      "outputs": [],
      "source": [
        "dqn_class = ALEDQN if type(env) == ALEEnvironment else MinAtarDQN\n",
        "summary(dqn_class(), input_shape=env.state_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163ErxValvhg"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_YRqk3yQw6M"
      },
      "source": [
        "Define training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9db1vQmYb0rK"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "  def __init__(self, size):\n",
        "    self.memory = deque([], maxlen=size)\n",
        "\n",
        "  def push(self, *args):\n",
        "    self.memory.append(Transition(*args))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)\n",
        "\n",
        "def train_dqn():\n",
        "  global dqn, dqn_target, optimizer, replay_memory, batch_size\n",
        "\n",
        "  if len(replay_memory) < batch_size:\n",
        "    return\n",
        "\n",
        "  # get batch of transitions from replay memory\n",
        "  transitions = replay_memory.sample(batch_size)\n",
        "  transition_batch = Transition(*zip(*transitions))\n",
        "\n",
        "  # get next states that are not final states\n",
        "  non_final_indices = torch.tensor([next_state is not None for next_state in transition_batch.next_state])\n",
        "  non_final_next_states = torch.cat([next_state for next_state in transition_batch.next_state if next_state is not None])\n",
        "\n",
        "  # separate transitions batch by state, action, reward\n",
        "  state_batch = torch.cat(transition_batch.state)\n",
        "  action_batch = torch.cat(transition_batch.action)\n",
        "  reward_batch = torch.cat(transition_batch.reward)\n",
        "  reward_batch[~non_final_indices] = -1.0\n",
        "\n",
        "  # get current q values and calculate expected q values\n",
        "  q_values = dqn(state_batch).gather(1, action_batch)\n",
        "  next_q_values = torch.zeros(batch_size, device=device)\n",
        "  next_q_values[non_final_indices] = dqn_target(non_final_next_states).max(1)[0].detach()\n",
        "  expected_q_values = (next_q_values * gamma) + reward_batch\n",
        "\n",
        "  # calculate loss\n",
        "  loss = criterion(q_values, expected_q_values.unsqueeze(1))\n",
        "\n",
        "  # update network\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  for param in dqn.parameters():\n",
        "    param.grad.data.clamp_(-1, 1)\n",
        "  optimizer.step()\n",
        "\n",
        "def select_action(state):\n",
        "  global epsilon, epsilon_start, epsilon_end, epsilon_random_frames, epsilon_greedy_frames, steps, dqn\n",
        "  steps += 1\n",
        "  if steps >= epsilon_random_frames:\n",
        "    epsilon -= (epsilon_start - epsilon_end) / epsilon_greedy_frames\n",
        "    epsilon = max(epsilon, epsilon_end)\n",
        "  if steps < epsilon_random_frames or epsilon > random.random():\n",
        "    return torch.tensor([[random.randrange(env.num_actions)]], device=device, dtype=torch.long)\n",
        "  else:\n",
        "     with torch.no_grad():\n",
        "      return dqn(state).max(1)[1].view(1, 1)\n",
        "\n",
        "def plot_stats():\n",
        "  global epsilon, episode_rewards, steps\n",
        "  \n",
        "  display.clear_output(wait=True)\n",
        "  \n",
        "  plt.title('Episode Total Rewards')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.plot(range(1, len(episode_rewards) + 1), episode_rewards, 'o', markersize=2)\n",
        "  plt.show()\n",
        "  \n",
        "  print(f'Total Episodes: {len(episode_rewards)} | Total Steps: {steps} | Max Score: {max(episode_rewards)} | Epsilon: {epsilon}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1AwpkNzHdui"
      },
      "source": [
        "Training settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wCCopGjHduj"
      },
      "outputs": [],
      "source": [
        "max_score = 21\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.02\n",
        "epsilon_random_frames = 50000\n",
        "epsilon_greedy_frames = 2000000\n",
        "replay_memory_size = 100000\n",
        "frame_skip = 8\n",
        "steps_per_target_update = 10000\n",
        "steps_per_dqn_update = 4\n",
        "steps_per_plot_update = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3lfVvY9Qur_"
      },
      "source": [
        "Train dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "hQdWCw8Vh1SL",
        "outputId": "09d61d41-0a2d-41e2-e812-dd04b4eb75db"
      },
      "outputs": [],
      "source": [
        "# initialize dqn and target dqn\n",
        "dqn = dqn_class().to(device)\n",
        "dqn_target = dqn_class().to(device)\n",
        "dqn_target.load_state_dict(dqn.state_dict())\n",
        "\n",
        "# initialize replay memory and training loss and optimizer\n",
        "replay_memory = ReplayMemory(replay_memory_size)\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = Adam(dqn.parameters(), lr=0.00025)\n",
        "\n",
        "steps = 0\n",
        "epsilon = epsilon_start\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in count():\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  for t in count():\n",
        "\n",
        "    # select action using epsilon greedy strategy\n",
        "    action = select_action(state)\n",
        "\n",
        "    # perform action and get reward\n",
        "    next_state, reward, done = env.step(action, frame_skip)\n",
        "    total_reward += reward\n",
        "\n",
        "    # add to replay memory\n",
        "    next_state = None if done else next_state\n",
        "    reward = torch.tensor([reward], device=device)\n",
        "    replay_memory.push(state, action, next_state, reward)\n",
        "    state = next_state\n",
        "\n",
        "    # train dqn on data saved to replay memory\n",
        "    if steps % steps_per_dqn_update == 0:\n",
        "      train_dqn()\n",
        "\n",
        "    # update target dqn with weights from main dqn\n",
        "    if steps % steps_per_plot_update == 0:\n",
        "      dqn_target.load_state_dict(dqn.state_dict())\n",
        "    \n",
        "    # check if game is over\n",
        "    if done:\n",
        "      episode_rewards.append(total_reward)\n",
        "      if episode % steps_per_plot_update == 0:\n",
        "        plot_stats()\n",
        "      break\n",
        "\n",
        "  # stop training once max score is reached\n",
        "  if total_reward >= max_score:\n",
        "    plot_stats()\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkHjZq0d26fH"
      },
      "source": [
        "Save trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cJG9GsB28lg"
      },
      "outputs": [],
      "source": [
        "torch.save(dqn.state_dict(), 'breakout_dqn.pt')\n",
        "torch.save(dqn_target.state_dict(), 'breakout_dqn_target.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZfP4x7q-5S6"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjgBliil5lSG"
      },
      "source": [
        "Load trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60z6iOiK5mx6"
      },
      "outputs": [],
      "source": [
        "dqn = dqn_class(env).to(device)\n",
        "dqn_target = dqn_class(env).to(device)\n",
        "\n",
        "dqn.load_state_dict(torch.load('breakout_dqn.pt'))\n",
        "dqn_target.load_state_dict(torch.load('breakout_dqn_target.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhgZCHh3S5Ot"
      },
      "source": [
        "Run game with trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgkrH_qWpCgE"
      },
      "outputs": [],
      "source": [
        "final_score = 21\n",
        "\n",
        "total_reward = 0\n",
        "while total_reward < final_score:\n",
        "  total_reward = 0\n",
        "  state = env.reset()\n",
        "  frames = [env.render()]\n",
        "\n",
        "  for t in count():\n",
        "    action = torch.argmax(dqn(state))\n",
        "    state, reward, done = env.step(action, frame_skip)\n",
        "    total_reward += reward\n",
        "    \n",
        "    frames.append(env.render())\n",
        "    \n",
        "    if done or total_reward >= final_score:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9me_hyEoviwI"
      },
      "source": [
        "Visualize gameplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc3_C0MlukEE"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.tight_layout(pad=0)\n",
        "ax.get_xaxis().set_visible(False)\n",
        "ax.get_yaxis().set_visible(False)\n",
        "ax.axis('off')\n",
        "\n",
        "def get_frame(index):\n",
        "  ax.clear()\n",
        "  return ax.imshow(frames[index], aspect=env.aspect, cmap=env.cmap)\n",
        "\n",
        "plt.close()\n",
        "animation.FuncAnimation(fig, get_frame, frames=len(frames), interval=50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Atari Reinforcement Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
